```bash
# RAGFlow Slim Environment Variables

# Flask Configuration
FLASK_ENV=production
RAGFLOW_API_KEY=changeme
RAGFLOW_LOG_LEVEL=INFO
RAGFLOW_LOG_FILE=runtime.log
RAGFLOW_CONFIG_DIR=/data/application

# Supabase Configuration
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-supabase-service-role-key

# Neo4j Configuration (for Graphiti)
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=graphiti_password

# ============================================================================
# LLM Provider Configuration (Multi-Provider Support)
# ============================================================================
# Supported providers: auto, ollama, google, openai
# - auto: Automatically detect available provider (Ollama > Google > OpenAI)
# - ollama: Use local Ollama (free, no API key needed)
# - google: Use Google AI (Gemini)
# - openai: Use OpenAI (GPT-4, GPT-3.5)

LLM_PROVIDER=auto

# --------------------------------------------------------------------------
# OpenAI Configuration (if using OpenAI or as fallback)
# --------------------------------------------------------------------------
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_MODEL=gpt-4o-mini  # Options: gpt-4o, gpt-4o-mini, gpt-3.5-turbo

# --------------------------------------------------------------------------
# Google AI Configuration (if using Google Gemini)
# --------------------------------------------------------------------------
GOOGLE_API_KEY=your-google-api-key-here
GOOGLE_MODEL=gemini-1.5-flash  # Options: gemini-1.5-flash, gemini-1.5-pro, gemini-pro

# --------------------------------------------------------------------------
# Ollama Configuration (if using local Ollama)
# --------------------------------------------------------------------------
# Download Ollama: https://ollama.ai
# Install models: ollama pull llama3.2 && ollama pull nomic-embed-text

OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2  # Options: llama3.2, llama3.1, mistral, phi3
OLLAMA_EMBED_MODEL=nomic-embed-text  # For embeddings

# ============================================================================
# Usage Examples by Scenario
# ============================================================================

# LOCAL DEVELOPMENT (Free with Ollama):
# LLM_PROVIDER=ollama
# OLLAMA_HOST=http://localhost:11434
# OLLAMA_MODEL=llama3.2

# TESTING WITH GOOGLE AI:
# LLM_PROVIDER=google
# GOOGLE_API_KEY=AIzaSy...your-key
# GOOGLE_MODEL=gemini-1.5-flash

# PRODUCTION WITH OPENAI:
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-proj-...your-key
# OPENAI_MODEL=gpt-4o-mini

# AUTO-DETECT (Recommended for flexibility):
# LLM_PROVIDER=auto
# # Set any API keys you have, system will use best available

# Optional: Docker-specific overrides
# When running with docker-compose, Neo4j URI should be:
# NEO4J_URI=bolt://ragflow-neo4j:7687
# When running Ollama in Docker:
# OLLAMA_HOST=http://host.docker.internal:11434

```
